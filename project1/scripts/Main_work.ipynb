{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Data loading\"\"\"\n",
    "def load_results(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    to_int = dict(s = 1,b = 0)\n",
    "    def convert(s):\n",
    "        return to_int.get(s.decode(\"utf-8\") , 0)\n",
    "    \n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1],\n",
    "                                converters={1: convert})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data_features(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, \n",
    "                                usecols=tuple(range(2,32)))\n",
    "    \n",
    "    ids = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0])\n",
    "    \n",
    "    return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    Extended = np.empty((x.shape[0],0))\n",
    "    \n",
    "    for j in range(0, degree+1):\n",
    "        for i in range(x.shape[1]):\n",
    "            Extended = np.c_[Extended, x[:,i]**j]\n",
    "          \n",
    "    \n",
    "    return Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(X, y, threshold = 0.01):\n",
    "    \"\"\"Removes features which are correlated with y with less than threshold\"\"\"\n",
    "    abs_corr = np.zeros(X.shape[1])\n",
    "    for index, x in enumerate(X.T):\n",
    "        abs_corr[index] = np.abs(np.corrcoef(y,x.T)[0,1])\n",
    "        \n",
    "    quality = np.where(abs_corr > threshold)\n",
    "    \n",
    "    return X[:,quality[0]], quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    # ***************************************************\n",
    "    w -= gradient*gamma\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, mean = None, sigma = None):\n",
    "    \"\"\"Standardizes the data\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(data[data != -999], axis = 0)\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = np.nanstd(data[data != -999], axis = 0)\n",
    "    \n",
    "    output = (data - mean)/sigma\n",
    "    \n",
    "    return output, mean, sigma\n",
    "\n",
    "def standardize_data(data, min_value = None, max_value = None):\n",
    "    \"\"\"maps data to [0,1] range\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = np.min(data, axis = 0)\n",
    "    \n",
    "    if max_value is None:\n",
    "        max_value = np.max(data, axis = 0)\n",
    "        \n",
    "    output = (data - min_value)/(max_value - min_value)\n",
    "    \n",
    "    return output, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(data, y):\n",
    "    \"\"\"Creates four subsets based on the number of jets,\n",
    "    which is 0, 1 and 2 or 3. 2 and 3 are put in one group,\n",
    "    since they keep same features and have similar correlation patterns\n",
    "    \"\"\"\n",
    "    data_subsets = []\n",
    "    y_subsets=[]\n",
    "    for i in range(3):\n",
    "        if i ==2:\n",
    "            mask = data[:,22] >= i\n",
    "        else:\n",
    "            mask = data[:,22] == i\n",
    "        data_subsets.append(data[mask])\n",
    "        y_subsets.append(y[mask])\n",
    "        \n",
    "    return data_subsets, y_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_variance(data, mask = None):\n",
    "    \"\"\"removes zero variance columns based on the subset\"\"\"\n",
    "    if mask is None:\n",
    "        variance = np.var(data, axis = 0)\n",
    "        mask = np.squeeze(~np.logical_or([variance ==0],[np.isnan(variance)]))\n",
    "        \n",
    "    return data[:, mask[:]], mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing(data, median = None):        \n",
    "    \"\"\"replaces nan by median value\"\"\"\n",
    "    if median is None:\n",
    "        median =[]\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            replace = np.median(data[mask,j])\n",
    "            data[~mask,j] = replace\n",
    "            median.append(replace)\n",
    "    else:\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            data[~mask,j] = median[j]\n",
    "\n",
    "    return data, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Processes the test and training data by:\n",
    "    -splitting data with respect to jet number, creating three groups\n",
    "    -removing zero variance in each subgroup\n",
    "    -removing columns which are lowly correlated to y\n",
    "    -normalizing the data with mean and standard devation \n",
    "    -replacing -999 by median value of column\"\"\"\n",
    "      \n",
    "    train_subsets, y_train = create_subsets(X_train, y_train)\n",
    "    test_subsets, y_test = create_subsets(X_test, y_test)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # change training sets\n",
    "        train_subsets[i], mean, sigma =  normalize_data(train_subsets[i], mean = None, sigma = None)\n",
    "        train_subsets[i], median = replace_missing(train_subsets[i], median = None)\n",
    "        train_subsets[i], mask = remove_zero_variance(train_subsets[i])\n",
    "        train_subsets[i], quality = correlation_filter(train_subsets[i], y_train[i], threshold = 0.01)\n",
    "        \n",
    "        #change test sets accordingly to training sets\n",
    "        test_subsets[i], _, _ =  normalize_data(test_subsets[i], mean = mean, sigma = sigma)\n",
    "        test_subsets[i], _ = replace_missing(test_subsets[i], median)\n",
    "        test_subsets[i], _ = remove_zero_variance(test_subsets[i], mask)\n",
    "        test_subsets[i] = test_subsets[i][:, quality]\n",
    "        \n",
    "    return train_subsets, test_subsets, y_train, y_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_terms(data):\n",
    "    \"\"\"Adds cross terms between columns\"\"\"\n",
    "    enriched_data = data\n",
    "    for x1 in data.T:\n",
    "        for x2 in data.T:\n",
    "            if np.sum(x1 - x2) != 0:\n",
    "                enriched_data = np.c_[enriched_data, x1*x2]\n",
    "                \n",
    "    return enriched_data      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_terms(data):\n",
    "    \"\"\"Adds log terms to data\"\"\"\n",
    "    extended = data\n",
    "    for column in data.T:\n",
    "        if np.sum(column <= -1) == 0:\n",
    "            extended = np.c_[extended, np.log(1+ column)]\n",
    "        \n",
    "    return extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data, degree = None, sqrt = True, log = True, cross_terms = True):\n",
    "    \"\"\"Adds following features to data set:\n",
    "    -log of features by log(1+x)\n",
    "    -sqrt of features\n",
    "    -polynomial extension of 0 up to degree\n",
    "    -cross terms of features\n",
    "    \"\"\" \n",
    "    #log\n",
    "    if log:\n",
    "        data = add_log_terms(data)\n",
    "        output = data\n",
    "    else:\n",
    "        output = np.empty((data.shape[0],0))\n",
    "        \n",
    "    #polynomial\n",
    "    if degree is not None:\n",
    "        output = np.c_[output, build_polynomial(data, degree)]\n",
    "      \n",
    "    # add sqrt\n",
    "    if sqrt:\n",
    "        output = np.c_[output, np.sqrt(np.abs(data))]\n",
    "        \n",
    "    \n",
    "    if cross_terms:\n",
    "        output = np.c_[output, add_cross_terms(data)]\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_solution(X_test, y_result):\n",
    "    \"\"\"Puts found y values back in right order for the complete data matrix,\n",
    "    since it was split in four groups.\n",
    "    X_test: original, preprocessed test data\n",
    "    y_result: output of created model, list of three vectors containing predictions for group 1,2 and 3\"\"\"\n",
    "    y_final=[]\n",
    "    index_1 = 0\n",
    "    index_2 = 0\n",
    "    index_3 = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if X_test[i,22] == 0:\n",
    "            y_final.append(y_result[0][index_1])\n",
    "            index_1 += 1\n",
    "            \n",
    "        elif X_test[i,22] == 1:\n",
    "            y_final.append(y_result[0][index_2])\n",
    "            index_2 += 1\n",
    "        \n",
    "        elif X_test[i,22] > 1:\n",
    "            y_final.append(y_result[0][index_3])\n",
    "            index_3 += 1\n",
    "         \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in greater\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#%% load data and process data\n",
    "path =  \"data\"\n",
    "X, _ = load_data_features(path +\"/train.csv\")\n",
    "y = load_results(path +\"/train.csv\")\n",
    "X_train, y_train = X[:int(0.8*len(X)),:], y[:int(0.8*len(X))]\n",
    "X_test, y_test = X[int(0.8*len(X)):,:], y[int(0.8*len(X)):]\n",
    "X_train, X_test_pro, y_train, y_test_pro = process_data(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "y_final = stitch_solution(X_test, y_result)\n",
    "print(len(y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for jet 0\n",
      "(1, 2) [[0. 0.]]\n",
      "Start ridge regression test for degree 1 ...\n",
      "Accuracy of the predictions is: 0.8414297889406929  and F-score is: 0.6539008644271802 with lambda: 1e-07\n",
      "Precision is: 0.7410714285714286 and recall is: 0.5850793029175642\n",
      "Start ridge regression test for degree 2 ...\n",
      "Accuracy of the predictions is: 0.8420815160174462  and F-score is: 0.6558881363338431 with lambda: 1.5e-07\n",
      "Precision is: 0.7417840375586855 and recall is: 0.587820638339534\n",
      "(2,) [0 0]\n",
      "Testing for jet 1\n",
      "(1, 2) [[0. 0.]]\n",
      "Start ridge regression test for degree 1 ...\n",
      "Accuracy of the predictions is: 0.7881767026712373  and F-score is: 0.6928055529500047 with lambda: 1e-07\n",
      "Precision is: 0.7156976744186047 and recall is: 0.6713324850027268\n",
      "Start ridge regression test for degree 2 ...\n",
      "Accuracy of the predictions is: 0.7899877110148115  and F-score is: 0.6951459956811566 with lambda: 1e-07\n",
      "Precision is: 0.7188349514563107 and recall is: 0.6729685511725141\n",
      "(2,) [1 1]\n",
      "Testing for jet 2\n",
      "(1, 2) [[0. 0.]]\n",
      "Start ridge regression test for degree 1 ...\n",
      "Accuracy of the predictions is: 0.8286732456140351  and F-score is: 0.8068603213844252 with lambda: 1e-07\n",
      "Precision is: 0.8173423070903115 and recall is: 0.7966437833714721\n",
      "Start ridge regression test for degree 2 ...\n",
      "Accuracy of the predictions is: 0.8317571271929824  and F-score is: 0.8113135039581892 with lambda: 1e-07\n",
      "Precision is: 0.8175340768277571 and recall is: 0.8051868802440885\n",
      "(2,) [1 1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ramge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-dce1846479fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbest_parameter_per_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_pro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mramge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_paramter_per_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_paramter_per_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ramge' is not defined"
     ]
    }
   ],
   "source": [
    "best_parameter_per_set = find_parameters(X_train, y_train, X_test_pro, y_test_pro)\n",
    "y_result = []\n",
    "for i in range(3):\n",
    "    lambda_ = best_parameter_per_set[i][0,0]\n",
    "    degree = int(best_parameter_per_set[i][0,1])\n",
    "    X_train_ex = add_features(X_train[i], degree = degree, sqrt = True, log = True, cross_terms = True)\n",
    "    X_test_pro_ex = add_features(X_test_pro[i], degree = degree, sqrt = True, log = True, cross_terms = True)\n",
    "    \n",
    "    show_result_ridge(X_train_ex, y_train[i], X_test_pro_ex, y_test_pro[i], lambda_)\n",
    "    \n",
    "    w = ridge_regression(y_train[i], X_train_ex, lambda_)\n",
    "    y_result.append(X_test_pro_ex @ w)\n",
    "\n",
    "y_final = stitch_solution(X_test, y_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parameters(X_train, y_train, X_test, y_test):\n",
    "    best_parameter_per_set = []\n",
    "    losses_sets =[]\n",
    "    degrees = np.linspace(1,22,21, dtype = int)\n",
    "    lambdas = np.logspace(-9,2,22)\n",
    "    degrees = [1, 2]\n",
    "    lambdas = [0.0000001, 0.00000015]\n",
    "    for i in range(3):\n",
    "        print(\"Testing for jet\",i)\n",
    "        parameters, losses = hyper_optimizing(X_train[i], y_train[i], X_test[i], y_test[i],\n",
    "                                methods = [\"Ridge_regression\"],\n",
    "                                 lambdas = lambdas, degrees = degrees)\n",
    "        \n",
    "        best_parameter_per_set.append(parameters)\n",
    "        losses_sets.append(losses)\n",
    "    \n",
    "    return best_parameter_per_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_optimizing(X_train, y_train, X_test, y_test, methods = [\"Ridge_regression\"],\n",
    "                     lambdas = [0.1], degrees = [1], gamma = 0.000001,  max_iter = 1):\n",
    "    \"\"\"Finds best lambda and degree to use on the given data, test possibilities are:\n",
    "    -Ridge regression\n",
    "    -Penalized Logistic regression\"\"\"\n",
    "    \n",
    "    # Check method names are correct\n",
    "    if len([i for i in methods if i in [\"Ridge_regression\", \"Penalized_logistic\"]]) < len(methods):\n",
    "        raise NameError(\"At least one method is wrong\")\n",
    "        \n",
    "    #seed = 1\n",
    "    #k_fold = 10\n",
    "    #k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    all_losses = np.zeros((len(methods), len(degrees), len(lambdas)))\n",
    "    best_parameters = np.zeros((len(methods), 2))\n",
    "    print(best_parameters.shape,best_parameters)\n",
    "    for degree_index, degree in enumerate(degrees):\n",
    "        X_train_ex = add_features(X_train, degree = degree, sqrt = True, log = True, cross_terms = True)\n",
    "    \n",
    "        for method_index, method in enumerate(methods):\n",
    "            \n",
    "            if method == \"Ridge_regression\":\n",
    "                seed = 1\n",
    "                k_fold = 10\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start ridge regression test for degree\", str(degree),\"...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te = cross_validation_ridge(y_train, X_train_ex, k_indices, k, lambda_)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = np.mean(losses_te)\n",
    "                 \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[np.argmin(all_losses[method_index, degree_index,:])]\n",
    "                X_test_ex = add_features(X_test, degree = degree, sqrt = True, log = True, cross_terms = True)\n",
    "                show_result_ridge(X_train_ex, y_train, X_test_ex, y_test, min_lambda)\n",
    "                    \n",
    "            elif method == \"Penalized_logistic\":\n",
    "                #less k-fold for reason of speed\n",
    "                seed = 1\n",
    "                k_fold = 1\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start penalized_logistic test...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te, _ = cross_validation_logistic(y_train, X_train_ex, k_indices,\n",
    "                                                    k, lambda_, gamma = 0.000001, max_iter = max_iter)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = np.mean(losses_te) \n",
    "                \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[np.argmin(all_losses[method_index, degree_index,:])]\n",
    "                X_test_ex = add_features(X_test, degree = degree, sqrt = True, log = True, cross_terms = True)\n",
    "                show_result_logistic(X_train_ex, y_train, X_test_ex, y_test, min_lambda, gamma = 0.000001)\n",
    "            \n",
    "            \n",
    "    print(all_losses)\n",
    "    for k  in range(len(methods)):\n",
    "        min_loss = np.argmin(all_losses[k,:,:], axis = 0)\n",
    "        print(min_loss.shape, min_loss)\n",
    "        best_parameters[k,0] = lambdas[min_loss[0]]\n",
    "        best_parameters[k,1] = degrees[min_loss[1]]\n",
    "        \n",
    "    return best_parameters, all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "    # ridge regression: \n",
    "    w = ridge_regression(y_train, x_train, lambda_)\n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test, x_test, w))\n",
    "    \n",
    "    return loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_ridge(X_train, y_train, X_test, y_test, lambda_):\n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    y_new[y_new<0.5]=0\n",
    "    y_new[y_new>=0.5]=1\n",
    "    summ = y_new + y_test\n",
    "    TP = sum(summ == 2)\n",
    "    TN = sum(summ == 0)\n",
    "    diff = y_new - y_test\n",
    "    FP = sum(diff == 1)\n",
    "    FN = sum(diff == -1)\n",
    "    accuracy = (TP+TN)/(TP +TN +FP + FN)\n",
    "    F_score = TP/(TP + 0.5 * (FP +FN))\n",
    "    recall = TP/(TP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n",
    "    print(\"Precision is:\",str(precision), \"and recall is:\",str(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_logistic(X_train, y_train, X_test, y_test, lambda_, gamma = 0.000001):\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    _, w = learning_by_penalized_gradient(y_train, X_train, w, gamma, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    # write function for this\n",
    "    y_new[y_new<0.5]=0\n",
    "    y_new[y_new>=0.5]=1\n",
    "    summ = y_new + y_test\n",
    "    TP = sum(summ == 2)\n",
    "    TN = sum(summ == 0)\n",
    "    diff = y_new - y_test\n",
    "    FP = sum(diff == 1)\n",
    "    FN = sum(diff == -1)\n",
    "    accuracy = (TP+TN)/(TP +TN +FP + FN)\n",
    "    F_score = TP/(TP + 0.5 * (FP +FN))\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k,lambda_, gamma,max_iter):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # split according to k_indices\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "    \n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y_train, x_train, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 999 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # check loss actually decreases, if not decrease gamma\n",
    "        if iter > 0:\n",
    "            if loss > losses[-1]:\n",
    "                gamma = gamma/2\n",
    "            \n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = calculate_loss(y_test, x_test, w)\n",
    "    \n",
    "    return loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    if len(tx.shape) > 1:\n",
    "        w = np.linalg.solve(tx.T @ tx + (2*tx.shape[0]*lambda_)*np.identity(tx.shape[1]), tx.T @ y)\n",
    "    else:\n",
    "        w = 1/(tx.T @ tx + lambda_) * tx.T @ y                        \n",
    "    # ***************************************************\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    z = tx @ w\n",
    "    a = np.sum(np.log(1 + np.exp(z)))\n",
    "    b = inter_y.T @ z\n",
    "    loss = a - b\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    gradient = tx.T @ (sigmoid(tx @ w) - inter_y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=43032.65641070308\n",
      "Current iteration=100, loss=38034.65434628601\n",
      "Current iteration=200, loss=37639.27664896377\n",
      "Current iteration=300, loss=37418.889681703076\n",
      "Current iteration=400, loss=37274.741199240416\n",
      "Current iteration=500, loss=37176.59226694897\n",
      "Current iteration=600, loss=37107.69305313046\n",
      "Current iteration=700, loss=37057.97530938601\n",
      "Current iteration=800, loss=37021.18935277108\n",
      "Current iteration=900, loss=36993.352056442345\n",
      "Current iteration=1000, loss=36971.86109215789\n",
      "Current iteration=1100, loss=36954.97566067417\n",
      "Current iteration=1200, loss=36941.504774259214\n",
      "Current iteration=1300, loss=36930.61588624264\n",
      "Current iteration=1400, loss=36921.71490249629\n",
      "Current iteration=1500, loss=36914.36945423421\n",
      "Current iteration=1600, loss=36908.25892183308\n",
      "Current iteration=1700, loss=36903.14131117387\n",
      "Current iteration=1800, loss=36898.83093163525\n",
      "Current iteration=1900, loss=36895.183110014565\n",
      "Current iteration=2000, loss=36892.083557627666\n",
      "Current iteration=2100, loss=36889.44085946411\n",
      "Current iteration=2200, loss=36887.1810872041\n",
      "Current iteration=2300, loss=36885.243876466\n",
      "Current iteration=2400, loss=36883.57952682233\n",
      "Current iteration=2500, loss=36882.14682560623\n",
      "Current iteration=2600, loss=36880.91139075083\n",
      "Current iteration=2700, loss=36879.84439091678\n",
      "Current iteration=2800, loss=36878.921543731034\n",
      "Current iteration=2900, loss=36878.12232197916\n",
      "Current iteration=3000, loss=36877.42931753753\n",
      "Current iteration=3100, loss=36876.827726653355\n",
      "Current iteration=3200, loss=36876.30492983953\n",
      "Current iteration=3300, loss=36875.85014646719\n",
      "Current iteration=3400, loss=36875.45414899972\n",
      "Current iteration=3500, loss=36875.10902532292\n",
      "Current iteration=3600, loss=36874.807980195794\n",
      "Current iteration=3700, loss=36874.54516875564\n",
      "Current iteration=3800, loss=36874.31555645012\n",
      "Current iteration=3900, loss=36874.11480087082\n",
      "Current iteration=4000, loss=36873.93915181725\n",
      "Current iteration=4100, loss=36873.78536659185\n",
      "Current iteration=4200, loss=36873.65063806072\n",
      "Current iteration=4300, loss=36873.53253344314\n",
      "Current iteration=4400, loss=36873.42894214022\n",
      "Current iteration=4500, loss=36873.33803119529\n",
      "Current iteration=4600, loss=36873.25820721118\n",
      "Current iteration=4700, loss=36873.188083740075\n",
      "Current iteration=4800, loss=36873.12645331977\n",
      "Current iteration=4900, loss=36873.07226346149\n",
      "Current iteration=5000, loss=36873.02459600366\n",
      "Current iteration=5100, loss=36872.982649336736\n",
      "Current iteration=5200, loss=36872.945723081204\n",
      "Current iteration=5300, loss=36872.913204863966\n",
      "Current iteration=5400, loss=36872.88455889267\n",
      "Current iteration=5500, loss=36872.859316072325\n",
      "Current iteration=5600, loss=36872.83706544673\n",
      "Current iteration=5700, loss=36872.81744677911\n",
      "Current iteration=5800, loss=36872.800144114386\n",
      "Current iteration=5900, loss=36872.78488018691\n",
      "Current iteration=6000, loss=36872.77141155877\n",
      "Current iteration=6100, loss=36872.759524389\n",
      "Current iteration=6200, loss=36872.74903074874\n",
      "Current iteration=6300, loss=36872.7397654091\n",
      "Current iteration=6400, loss=36872.73158303907\n",
      "Current iteration=6500, loss=36872.72435575904\n",
      "Current iteration=6600, loss=36872.71797100321\n",
      "Current iteration=6700, loss=36872.71232965081\n",
      "Current iteration=6800, loss=36872.70734439096\n",
      "Current iteration=6900, loss=36872.7029382912\n",
      "Current iteration=7000, loss=36872.699043543485\n",
      "Current iteration=7100, loss=36872.69560036504\n",
      "Current iteration=7200, loss=36872.69255603438\n",
      "Current iteration=7300, loss=36872.68986404539\n",
      "Current iteration=7400, loss=36872.68748336483\n",
      "Current iteration=7500, loss=36872.68537777993\n",
      "Current iteration=7600, loss=36872.683515325305\n",
      "Current iteration=7700, loss=36872.681867778985\n",
      "Current iteration=7800, loss=36872.6804102193\n",
      "Current iteration=7900, loss=36872.67912063487\n",
      "Current iteration=8000, loss=36872.677979581495\n",
      "Current iteration=8100, loss=36872.67696987988\n",
      "Current iteration=8200, loss=36872.67607634957\n",
      "Current iteration=8300, loss=36872.67528557433\n",
      "Current iteration=8400, loss=36872.6745856956\n",
      "Current iteration=8500, loss=36872.67396623027\n",
      "Current iteration=8600, loss=36872.67341790995\n",
      "Current iteration=8700, loss=36872.67293253934\n",
      "Current iteration=8800, loss=36872.67250287115\n",
      "Current iteration=8900, loss=36872.67212249582\n",
      "Current iteration=9000, loss=36872.67178574411\n",
      "Current iteration=9100, loss=36872.67148760115\n",
      "Current iteration=9200, loss=36872.67122363053\n",
      "Current iteration=9300, loss=36872.670989907194\n",
      "Current iteration=9400, loss=36872.67078295834\n",
      "Current iteration=9500, loss=36872.670599710975\n",
      "Current iteration=9600, loss=36872.67043744579\n",
      "Current iteration=9700, loss=36872.67029375625\n",
      "Current iteration=9800, loss=36872.670166512486\n",
      "Current iteration=9900, loss=36872.670053829395\n",
      "0.7148955436258975\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((X_train[1].shape[1], 1))\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "max_iter = 10000\n",
    "lambda_ = 0\n",
    "gamma = 0.000001\n",
    "#start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "     # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_train[1], X_train[1], w, gamma, lambda_)\n",
    "        # log info\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break\n",
    "\n",
    "y_new = X_test[1] @ w\n",
    "y_new[y_new<0.5]=0\n",
    "y_new[y_new>=0.5]=1\n",
    "correct = y_new.T - y_test[1]\n",
    "correct[correct!=0]=1\n",
    "print(1 - np.sum(correct)/len(correct[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma, lambda_):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "            # ***************************************************\n",
    "            # update w by gradient\n",
    "            w -= gamma * gradient   \n",
    "            \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        if n_iter % 100 == 0:\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                  bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "            \n",
    "    # ***************************************************\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/2999): loss=43032.65641070308, w0=[-0.00015543], w1=[0.00057892]\n",
      "Gradient Descent(100/2999): loss=38308.48336908368, w0=[-0.01301644], w1=[0.0051777]\n",
      "Gradient Descent(200/2999): loss=38063.16392576093, w0=[-0.02341128], w1=[0.00713485]\n",
      "Gradient Descent(300/2999): loss=37967.2307168726, w0=[-0.03316536], w1=[0.00854527]\n",
      "Gradient Descent(400/2999): loss=37894.802331778694, w0=[-0.04267698], w1=[0.00971019]\n",
      "Gradient Descent(500/2999): loss=37831.9663784284, w0=[-0.05205284], w1=[0.0107498]\n",
      "Gradient Descent(600/2999): loss=37775.452820463935, w0=[-0.06132667], w1=[0.01172069]\n",
      "Gradient Descent(700/2999): loss=37723.94678832, w0=[-0.07051208], w1=[0.01265249]\n",
      "Gradient Descent(800/2999): loss=37676.72611830914, w0=[-0.0796165], w1=[0.01356169]\n",
      "Gradient Descent(900/2999): loss=37633.29941362311, w0=[-0.08864521], w1=[0.01445768]\n",
      "Gradient Descent(1000/2999): loss=37593.28328453124, w0=[-0.09760252], w1=[0.01534588]\n",
      "Gradient Descent(1100/2999): loss=37556.35561975944, w0=[-0.10649221], w1=[0.01622941]\n",
      "Gradient Descent(1200/2999): loss=37522.235766380945, w0=[-0.11531769], w1=[0.01711]\n",
      "Gradient Descent(1300/2999): loss=37490.67485456924, w0=[-0.12408209], w1=[0.01798858]\n",
      "Gradient Descent(1400/2999): loss=37461.45022094635, w0=[-0.13278831], w1=[0.01886561]\n",
      "Gradient Descent(1500/2999): loss=37434.36165330072, w0=[-0.14143908], w1=[0.01974125]\n",
      "Gradient Descent(1600/2999): loss=37409.22855542303, w0=[-0.15003691], w1=[0.02061553]\n",
      "Gradient Descent(1700/2999): loss=37385.887650907505, w0=[-0.15858418], w1=[0.02148835]\n",
      "Gradient Descent(1800/2999): loss=37364.19104922151, w0=[-0.16708313], w1=[0.02235957]\n",
      "Gradient Descent(1900/2999): loss=37344.00458180616, w0=[-0.17553587], w1=[0.02322904]\n",
      "Gradient Descent(2000/2999): loss=37325.20635326736, w0=[-0.18394437], w1=[0.02409659]\n",
      "Gradient Descent(2100/2999): loss=37307.685470723074, w0=[-0.1923105], w1=[0.02496205]\n",
      "Gradient Descent(2200/2999): loss=37291.3409240732, w0=[-0.20063604], w1=[0.02582526]\n",
      "Gradient Descent(2300/2999): loss=37276.080595831845, w0=[-0.20892265], w1=[0.02668606]\n",
      "Gradient Descent(2400/2999): loss=37261.820383115526, w0=[-0.21717191], w1=[0.02754432]\n",
      "Gradient Descent(2500/2999): loss=37248.4834172669, w0=[-0.22538533], w1=[0.02839989]\n",
      "Gradient Descent(2600/2999): loss=37235.99936882479, w0=[-0.23356431], w1=[0.02925266]\n",
      "Gradient Descent(2700/2999): loss=37224.303827340496, w0=[-0.24171022], w1=[0.0301025]\n",
      "Gradient Descent(2800/2999): loss=37213.337747010475, w0=[-0.24982433], w1=[0.03094932]\n",
      "Gradient Descent(2900/2999): loss=37203.04695032167, w0=[-0.25790786], w1=[0.03179301]\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((X_test[1].shape[1], 1))\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "max_iter = 3000\n",
    "lambda_ = 0.08\n",
    "gamma = 0.00001\n",
    "batch_size = 5000\n",
    "max_iters = 1000\n",
    "\n",
    "losses, ws = stochastic_gradient_descent(y_train[1], X_train[1], w, batch_size, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= y.reshape(250000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+np.exp(-227.91778431))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.65116124e+05, 1.24925594e+03, 1.66697530e+03, 4.05202959e+03,\n",
       "       2.06551758e+05, 4.32925819e+05, 2.05226188e+05, 6.12947368e-01,\n",
       "       4.96106539e+02, 1.33878515e+04, 7.13587788e-01, 1.42463906e+00,\n",
       "       2.05749162e+05, 5.02299351e+02, 1.47398106e+00, 3.30061476e+00,\n",
       "       4.86858853e+02, 1.60017344e+00, 3.30006328e+00, 1.08205651e+03,\n",
       "       3.28413798e+00, 1.60020609e+04, 9.55358361e-01, 2.84048199e+05,\n",
       "       2.39451000e+05, 2.39446692e+05, 2.30279570e+05, 2.05556795e+05,\n",
       "       2.05560779e+05, 9.60703157e+03])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(X, axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
