{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Data loading\"\"\"\n",
    "def load_results(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    to_int = dict(s = 1,b = 0)\n",
    "    def convert(s):\n",
    "        return to_int.get(s.decode(\"utf-8\") , 0)\n",
    "    \n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1],\n",
    "                                converters={1: convert})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data_features(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, \n",
    "                                usecols=tuple(range(2,32)))\n",
    "    \n",
    "    ids = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0])\n",
    "    \n",
    "    return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    Extended = np.empty((x.shape[0],0))\n",
    "    \n",
    "    for j in range(0, degree+1):\n",
    "        for i in range(x.shape[1]):\n",
    "            Extended = np.c_[Extended, x[:,i]**j]\n",
    "          \n",
    "    \n",
    "    return Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(X, y):\n",
    "    abs_corr = np.zeros(X.shape[1])\n",
    "    for index, x in enumerate(X.T):\n",
    "        abs_corr[index] = np.abs(np.corrcoef(y,x.T)[0,1])\n",
    "    quality = np.where(abs_corr > 0.05)\n",
    "    return X[:,quality[0]], quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    # ***************************************************\n",
    "    w -= gradient*gamma\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data, mean = None, sigma = None, min_value = None, max_value = None):\n",
    "    \"\"\"Standardizes the data and maps data to [0,1] range to prevent overflows\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(data, axis = 0)\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = np.nanstd(data, axis = 0)\n",
    "    \n",
    "    output = (data - mean)/sigma\n",
    "    \n",
    "    if min_value is None:\n",
    "        min_value = np.min(data, axis = 0)\n",
    "    \n",
    "    if max_value is None:\n",
    "        max_value = np.max(data, axis = 0)\n",
    "        \n",
    "    output = (output - min_value)/(max_value - min_value)\n",
    "    \n",
    "    return output, mean, sigma, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(data, y):\n",
    "    \"\"\"Creates four subsets based on the number of jets, which is 0, 1, 2 or 3\"\"\"\n",
    "    data_subsets = []\n",
    "    y_subsets=[]\n",
    "    for i in range(4):\n",
    "        mask = data[:,22] == i\n",
    "        data_subsets.append(data[mask])\n",
    "        y_subsets.append(y[mask])\n",
    "        \n",
    "    return data_subsets, y_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_variance(data, mask = None):\n",
    "    \"\"\"removes zero variance columns based on the subset\"\"\"\n",
    "    if mask is None:\n",
    "        variance = np.var(data, axis = 0)\n",
    "        mask = np.squeeze(~np.logical_or([variance ==0],[np.isnan(variance)]))\n",
    "        \n",
    "    return data[:, mask[:]], mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_or_remove_nan(data, delete = None):        \n",
    "    \"\"\"deletes columns with nan only and replaces nan by median value\"\"\"\n",
    "    \n",
    "    if delete is None:\n",
    "        delete = []\n",
    "        for j in range(data.shape[1]):\n",
    "            if np.all(np.isnan(data[:,j])):\n",
    "                delete.append(j)\n",
    "                \n",
    "    data = np.delete(data, delete, axis = 1)\n",
    "\n",
    "    for j in range(data.shape[1]):\n",
    "        replace = np.median(data[:,j])\n",
    "        data[np.isnan(data[:,j]),j] = replace\n",
    "\n",
    "    \n",
    "    return data, delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Processes the test and training data, splits with respect to jet number,\n",
    "    removes zero variance, standardizes and normalizes the data and \n",
    "    replaces -999 by median value of column\"\"\"\n",
    "    \n",
    "    train_subsets, y_train = create_subsets(X_train, y_train)\n",
    "    test_subsets, y_test = create_subsets(X_test, y_test)\n",
    "    \n",
    "    for i in range(4):\n",
    "        train_subsets[i], mask = remove_zero_variance(train_subsets[i])\n",
    "        train_subsets[i][train_subsets[i] == -999] = np.nan\n",
    "        train_subsets[i], mean, sigma, min_value, max_value =  standardize_data(train_subsets[i],\n",
    "                                                                               mean = None, sigma = None, min_value = None, max_value = None)\n",
    "        \n",
    "        train_subsets[i], delete = replace_or_remove_nan(train_subsets[i], delete = None)\n",
    "        \n",
    "        test_subsets[i], _ = remove_zero_variance(test_subsets[i], mask)\n",
    "        test_subsets[i][test_subsets[i] == -999] =np.nan\n",
    "        test_subsets[i], _, _, _, _ =  standardize_data(test_subsets[i],\n",
    "                                                mean = mean, sigma = sigma, min_value = min_value, max_value = max_value)\n",
    "        test_subsets[i], _ = replace_or_remove_nan(test_subsets[i], delete = delete)\n",
    "        \n",
    "        \n",
    "    return train_subsets, test_subsets, y_train, y_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data, degree = None, sqrt = True, log = True):\n",
    "    \"\"\"Adds following features to data set:\n",
    "    -sqrt of features\n",
    "    -polynomial extension of 0 up to degree\n",
    "    -log of features\n",
    "    \"\"\" \n",
    "    output = np.empty((data.shape[0],0))\n",
    "    # add sqrt\n",
    "    if sqrt:\n",
    "        output = np.c_[output, np.sqrt(np.abs(data))]\n",
    "        \n",
    "    #polynomial\n",
    "    if degree is not None:\n",
    "        output = np.c_[output, build_polynomial(data, degree)]\n",
    "        \n",
    "    #log\n",
    "    if log:\n",
    "        output = np.c_[output, np.log(np.abs(data))]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load data and process data\n",
    "path =  \"data\"\n",
    "X, _ = load_data_features(path +\"/train.csv\")\n",
    "y = load_results(path +\"/train.csv\")\n",
    "X_test, y_test = X[:int(0.8*len(X)),:], y[:int(0.8*len(X))]\n",
    "X_train, y_train = X[int(0.8*len(X)):,:], y[int(0.8*len(X)):]\n",
    "X_train, X_test, y_train, y_test = process_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10716.748558637313\n",
      "Current iteration=1000, loss=568885.5464600606\n",
      "Current iteration=2000, loss=636965.8299172868\n",
      "Current iteration=3000, loss=151355.72751964434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-326-3e2db8cb797c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m      \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6ab66d51d99a>\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# ***************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-321-a59f5bc337c8>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0minter_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minter_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "ff = add_features(X_train[1], degree = 15, sqrt = True, log = True)\n",
    "xx = add_features(X_test[1], degree = 15, sqrt = True, log = True)\n",
    "w = np.zeros((ff.shape[1], 1))\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "max_iter = 100000\n",
    "lambda_ = 0.5\n",
    "gamma = 0.00001\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "     # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_train[1], ff, w, gamma, lambda_)\n",
    "        # log info\n",
    "    if iter % 1000 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([738.])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = xx @ w\n",
    "y[y<0.5] = 0\n",
    "y[y>=0.5] = 1\n",
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff = add_features(X_train[1], degree = 15, sqrt = True, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    z = tx @ w\n",
    "    a = np.sum(np.log(1 + np.exp(z)))\n",
    "    b = inter_y.T @ z\n",
    "    loss = a - b\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    gradient = tx.T @ (sigmoid(tx @ w) - inter_y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "            \n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma, lambda_):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "            # ***************************************************\n",
    "            # update w by gradient\n",
    "            w -= gamma * gradient   \n",
    "            \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        if n_iter % 100 == 0:\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                  bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "            \n",
    "    # ***************************************************\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/299): loss=173286.79513998624, w0=[0.0120085], w1=[-0.41697064]\n",
      "Gradient Descent(100/299): loss=151768.76755397083, w0=[0.01630981], w1=[-0.67932002]\n",
      "Gradient Descent(200/299): loss=151025.752212893, w0=[0.01256946], w1=[-0.6797545]\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros((X_new.shape[1], 1))\n",
    "threshold = 1e-8\n",
    "losses = []\n",
    "max_iter = 300\n",
    "lambda_ = 0.08\n",
    "gamma = 0.00001\n",
    "batch_size = 5000\n",
    "max_iters = 1000\n",
    "\n",
    "losses, ws = stochastic_gradient_descent(y, X_new, w, batch_size, max_iter, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= y.reshape(250000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+np.exp(-227.91778431))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.65116124e+05, 1.24925594e+03, 1.66697530e+03, 4.05202959e+03,\n",
       "       2.06551758e+05, 4.32925819e+05, 2.05226188e+05, 6.12947368e-01,\n",
       "       4.96106539e+02, 1.33878515e+04, 7.13587788e-01, 1.42463906e+00,\n",
       "       2.05749162e+05, 5.02299351e+02, 1.47398106e+00, 3.30061476e+00,\n",
       "       4.86858853e+02, 1.60017344e+00, 3.30006328e+00, 1.08205651e+03,\n",
       "       3.28413798e+00, 1.60020609e+04, 9.55358361e-01, 2.84048199e+05,\n",
       "       2.39451000e+05, 2.39446692e+05, 2.30279570e+05, 2.05556795e+05,\n",
       "       2.05560779e+05, 9.60703157e+03])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(X, axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
