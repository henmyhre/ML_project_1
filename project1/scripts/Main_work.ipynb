{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    to_int = dict(s = 1,b = 0)\n",
    "    def convert(s):\n",
    "        return to_int.get(s.decode(\"utf-8\") , 0)\n",
    "    \n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1],\n",
    "                                converters={1: convert})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data_features(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    data = data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, \n",
    "                                usecols=tuple(range(2,32)))\n",
    "    \n",
    "    ids = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0])\n",
    "    \n",
    "    return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    Extended = np.empty((x.shape[0],0))\n",
    "    \n",
    "    for j in range(0, degree+1):\n",
    "        for i in range(x.shape[1]):\n",
    "            Extended = np.c_[Extended, x[:,i]**j]\n",
    "    \n",
    "    return Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(X, y, threshold = 0.01):\n",
    "    \"\"\"Removes features which are correlated with y with less than threshold\"\"\"\n",
    "    abs_corr = np.zeros(X.shape[1])\n",
    "    for index, x in enumerate(X.T):\n",
    "        abs_corr[index] = np.abs(np.corrcoef(y,x.T)[0,1])\n",
    "        \n",
    "    quality = np.where(abs_corr > threshold)\n",
    "    \n",
    "    return X[:,quality[0]], quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, mean = None, sigma = None):\n",
    "    \"\"\"Standardizes the data\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(data[data != -999], axis = 0)\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = np.nanstd(data[data != -999], axis = 0)\n",
    "    \n",
    "    output = (data - mean)/sigma\n",
    "    \n",
    "    return output, mean, sigma\n",
    "\n",
    "def standardize_data(data, min_value = None, max_value = None):\n",
    "    \"\"\"maps data to [0,1] range\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = np.min(data, axis = 0)\n",
    "    \n",
    "    if max_value is None:\n",
    "        max_value = np.max(data, axis = 0)\n",
    "        \n",
    "    output = (data - min_value)/(max_value - min_value)\n",
    "    \n",
    "    return output, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(data, y, ids):\n",
    "    \"\"\"Creates four subsets based on the number of jets,\n",
    "    which is 0, 1 and 2 or 3. 2 and 3 are put in one group,\n",
    "    since they keep same features and have similar correlation patterns\n",
    "    \"\"\"\n",
    "    data_subsets = []\n",
    "    y_subsets = []\n",
    "    ids_subsets = []\n",
    "    for i in range(3):\n",
    "        if i ==2:\n",
    "            mask = data[:,22] >= i\n",
    "        else:\n",
    "            mask = data[:,22] == i\n",
    "        data_subsets.append(data[mask])\n",
    "        if y is not None:\n",
    "            y_subsets.append(y[mask])\n",
    "            \n",
    "        ids_subsets.append(ids[mask])\n",
    "        \n",
    "    return data_subsets, y_subsets, ids_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_variance(data, mask = None):\n",
    "    \"\"\"removes zero variance columns based on the subset\"\"\"\n",
    "    if mask is None:\n",
    "        variance = np.var(data, axis = 0)\n",
    "        mask = variance == 0\n",
    "    return data[:, ~mask[:]], mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing(data, median = None):        \n",
    "    \"\"\"replaces nan by median value\"\"\"\n",
    "    if median is None:\n",
    "        median =[]\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            replace = np.median(data[mask,j])\n",
    "            data[~mask,j] = replace\n",
    "            median.append(replace)\n",
    "    else:\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            data[~mask,j] = median[j]\n",
    "\n",
    "    return data, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X_train, X_test, y_train, y_test, ids_train, ids_test):\n",
    "    \"\"\"\n",
    "    Processes the test and training data by:\n",
    "    -splitting data with respect to jet number, creating three groups\n",
    "    -removing zero variance in each subgroup\n",
    "    -removing columns which are lowly correlated to y\n",
    "    -normalizing the data with mean and standard devation \n",
    "    -replacing -999 by median value of column\n",
    "    \"\"\"\n",
    "      \n",
    "    train_subsets, y_train, ids_train = create_subsets(X_train, y_train, ids_train)\n",
    "    test_subsets, y_test, ids_test = create_subsets(X_test, y_test, ids_test)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # change training sets\n",
    "        train_subsets[i], mask = remove_zero_variance(train_subsets[i])\n",
    "        print(\"For subgroup\",i,\"The following columns were removed due to zero variance:\",[i for i, x in enumerate(mask) if x])\n",
    "        \n",
    "        train_subsets[i], mean, sigma = normalize_data(train_subsets[i], mean = None, sigma = None) \n",
    "        train_subsets[i], median = replace_missing(train_subsets[i], median = None) \n",
    "        train_subsets[i], quality = correlation_filter(train_subsets[i], y_train[i], threshold = 0.01)\n",
    "        print(\"For subgroup\",i,\"The following columns were kept after low correlation:\",quality)\n",
    "        print(\"Final shape of subset\",i,\"is:\",train_subsets[i].shape)\n",
    "        \n",
    "        #change test sets accordingly to training sets\n",
    "        test_subsets[i], _ = remove_zero_variance(test_subsets[i], mask)\n",
    "        test_subsets[i], _, _ =  normalize_data(test_subsets[i], mean, sigma)\n",
    "        test_subsets[i], _ = replace_missing(test_subsets[i], median)\n",
    "        test_subsets[i] = test_subsets[i][:, quality]\n",
    "        \n",
    "    return train_subsets, test_subsets, y_train, y_test, ids_train, ids_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_terms(data):\n",
    "    \"\"\"Adds cross terms between columns\"\"\"\n",
    "    enriched_data = data\n",
    "    for x1 in data.T:\n",
    "        for x2 in data.T:\n",
    "            if np.sum(x1 - x2) != 0:\n",
    "                enriched_data = np.c_[enriched_data, x1*x2]\n",
    "                \n",
    "    return enriched_data      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_terms(data):\n",
    "    \"\"\"Adds log terms to data\"\"\"\n",
    "    extended = data\n",
    "    for column in data.T:\n",
    "        if np.sum(column <= -1) == 0:\n",
    "            extended = np.c_[extended, np.log(1+ column)]\n",
    "        \n",
    "    return extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data, degree = None, sqrt = True, log = True, cross_terms = True):\n",
    "    \"\"\"\n",
    "    Adds following features to data set:\n",
    "    -log of features by log(1+x)\n",
    "    -sqrt of features\n",
    "    -polynomial extension of 0 up to degree\n",
    "    -cross terms of features\n",
    "    \"\"\" \n",
    "    #log\n",
    "    if log:\n",
    "        data = add_log_terms(data)\n",
    "        output = data\n",
    "    else:\n",
    "        output = np.empty((data.shape[0],0))\n",
    "        \n",
    "    #polynomial\n",
    "    if degree is not None:\n",
    "        output = np.c_[output, build_polynomial(data, degree)]\n",
    "      \n",
    "    # add sqrt\n",
    "    if sqrt:\n",
    "        output = np.c_[output, np.sqrt(np.abs(data))]\n",
    "        \n",
    "    \n",
    "    if cross_terms:\n",
    "        output = np.c_[output, add_cross_terms(data)]\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_solution(X_test, y_result, ids_test_group, ids_test):\n",
    "    \"\"\"\n",
    "    Puts found y values back in right order for the complete data matrix,\n",
    "    since it was split in four groups.\n",
    "    X_test: original, preprocessed test data\n",
    "    y_result: output of created model, list of three vectors containing predictions for group 1,2 and 3\n",
    "    \"\"\"\n",
    "    y_final=[]\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if ids_test[i] in ids_test_group[0]:\n",
    "            index = np.where(ids_test_group[0] == ids_test[i])\n",
    "            y_final.append(y_result[0][index])\n",
    "            \n",
    "        elif ids_test[i] in ids_test_group[1]:\n",
    "            index = np.where(ids_test_group[1] == ids_test[i])\n",
    "            y_final.append(y_result[1][index])\n",
    "            \n",
    "        elif ids_test[i] in ids_test_group[2]:\n",
    "            index = np.where(ids_test_group[2] == ids_test[i])\n",
    "            y_final.append(y_result[2][index])\n",
    "            \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_optimizing(X_train, y_train, methods = [\"Ridge_regression\"],\n",
    "                     lambdas = [0.1], degrees = [1], gamma = 0.0000001,  max_iter = 3000):\n",
    "    \"\"\"Finds best lambda and degree to use on the given data, test possibilities are:\n",
    "    -Ridge regression\n",
    "    -Penalized Logistic regression\"\"\"\n",
    "    \n",
    "    # Check method names are correct\n",
    "    if len([i for i in methods if i in [\"Ridge_regression\", \"Penalized_logistic\"]]) < len(methods):\n",
    "        raise NameError(\"At least one method is wrong\")\n",
    "        \n",
    "    all_losses = np.zeros((len(methods), len(degrees), len(lambdas)))\n",
    "    best_parameters = np.zeros((len(methods), 2))\n",
    "    \n",
    "    for degree_index, degree in enumerate(degrees):\n",
    "        X_train_ex = add_features(X_train, degree = degree)\n",
    "    \n",
    "        for method_index, method in enumerate(methods):\n",
    "            \n",
    "            if method == \"Ridge_regression\":\n",
    "                seed = 1\n",
    "                k_fold = 5\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start ridge regression test for degree\", str(degree),\"...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te = cross_validation_ridge(y_train, X_train_ex, k_indices, k, lambda_)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = np.mean(losses_te)\n",
    "                 \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[np.argmin(all_losses[method_index, degree_index,:])]\n",
    "                print(\"Lowest loss is for lambda:\", min_lambda, \"is:\", min(all_losses[method_index, degree_index,:]))\n",
    "                \n",
    "                \n",
    "            elif method == \"Penalized_logistic\":\n",
    "                #less k-fold for reason of speed\n",
    "                seed = 1\n",
    "                k_fold = 4\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start penalized_logistic test...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te, _ = cross_validation_logistic(y_train, X_train_ex, k_indices,\n",
    "                                                    k, lambda_, gamma, max_iter)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = np.mean(losses_te) \n",
    "                \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[np.argmin(all_losses[method_index, degree_index,:])]\n",
    "                print(\"Lowest loss is:\", min(all_losses[method_index, degree_index,:]),\"for lambda:\", min_lambda)\n",
    "                \n",
    "            \n",
    "    min_loss = np.argmin(all_losses)\n",
    "    min_loss = np.unravel_index(min_loss, (len(methods), len(degrees), len(lambdas)))\n",
    "    best_parameters[0] = methods[min_loss[0]]\n",
    "    best_parameters[1] = degrees[min_loss[1]]\n",
    "    best_parameters[2] = lambdas[min_loss[2]]  \n",
    "    \n",
    "    return best_parameters, all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_result(y_found, y_real):\n",
    "    y_found[y_found<0.5]=0\n",
    "    y_found[y_found>=0.5]=1\n",
    "    summ = y_found + y_real\n",
    "    TP = np.sum(summ == 2)\n",
    "    TN = np.sum(summ == 0)\n",
    "    diff = y_found - y_real\n",
    "    FP = np.sum(diff == 1)\n",
    "    FN = np.sum(diff == -1)\n",
    "    accuracy = (TP+TN)/(TP +TN +FP + FN)\n",
    "    F_score = TP/(TP + 0.5 * (FP +FN))\n",
    "    recall = TP/(TP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    return precision, recall, F_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "\n",
    "    w = ridge_regression(y_train, x_train, lambda_)\n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test, x_test, w))\n",
    "    # Calculate F_score, seems more reliable to compare different degrees\n",
    "    #y_new = x_test @ w\n",
    "    #precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    \n",
    "    return loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"ridge regression\"\"\"\n",
    "    if len(tx.shape) > 1:\n",
    "        w = np.linalg.solve(tx.T @ tx + (2*tx.shape[0]*lambda_)*np.identity(tx.shape[1]), tx.T @ y)\n",
    "    else:\n",
    "        w = 1/(tx.T @ tx + lambda_) * tx.T @ y                        \n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k,lambda_, gamma,max_iter):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # split according to k_indices\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "    \n",
    "    w = np.random.randn(x.shape[1], 1)\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    # start the logistic regression\n",
    "    iter = 0\n",
    "    while iter <max_iter:\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y_train, x_train, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 999 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "        # check loss actually decreases, if not decrease gamma\n",
    "        if iter > 0:\n",
    "            if loss > losses[-1]:\n",
    "                gamma = gamma/2\n",
    "            if np.isinf(loss):\n",
    "                iter = 0\n",
    "                w = np.random.randn(x.shape[1], 1)\n",
    "                gamma = gamma/10\n",
    "                \n",
    "        iter +=1\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = calculate_loss(y_test, x_test, w)\n",
    "    \n",
    "    # Calculate F_score, seems more reliable to compare different degrees\n",
    "    y_new = x_test @ w\n",
    "    precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    print(accuracy, F_score)\n",
    "    \n",
    "    return loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    w -= gradient*gamma\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"applies the sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"computes the loss: negative log likelihood.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    z = tx @ w\n",
    "    a = np.sum(np.log(1 + np.exp(z)))\n",
    "    b = inter_y.T @ z\n",
    "    loss = a - b\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"computes the gradient of loss.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    gradient = tx.T @ (sigmoid(tx @ w) - inter_y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_ridge(X_train, y_train, X_test, y_test, lambda_):\n",
    "    \"\"\"prints accuracy of ridge regression\"\"\"\n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n",
    "    print(\"Precision is:\",str(precision), \"and recall is:\",str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_logistic(X_train, y_train, X_test, y_test, lambda_, gamma = 0.000001):\n",
    "    \"\"\"prints accuracy of logistic regression\"\"\"\n",
    "    w = np.zeros((X_train.shape[1], 1))\n",
    "    _, w = learning_by_penalized_gradient(y_train, X_train, w, gamma, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n",
    "    print(\"Precision is:\",str(precision), \"and recall is:\",str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parameters(X_train, y_train):\n",
    "    \"\"\"Finding best parameters and losses per set\"\"\"\n",
    "    best_parameter_per_set = []\n",
    "    losses_sets =[]\n",
    "    methods = [\"Penalized_logistic\"]\n",
    "    degrees = np.linspace(1,5,5, dtype = int)\n",
    "    lambdas = np.logspace(-13,-9,4)\n",
    "    lambdas = np.append(lambdas, 0)\n",
    "    for i in range(1):\n",
    "        print(\"Testing for set\",i)\n",
    "        parameters, losses = hyper_optimizing(X_train[i], y_train[i],\n",
    "                                methods, lambdas, degrees, 10**-10, 3000)\n",
    "        \n",
    "        best_parameter_per_set.append(parameters)\n",
    "        losses_sets.append(losses)\n",
    "    \n",
    "    return best_parameter_per_set, losses_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(3,3,3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.e-13, 3.e+00]])]\n",
      "[array([[[10940.94732142, 10940.94732142, 10940.94732142, 10940.94732142],\n",
      "        [11419.25563444, 11419.25563444, 11419.25563444, 11419.25563444],\n",
      "        [11399.85266053, 11399.85266053, 11399.85266053, 11399.85266053],\n",
      "        [11396.33972425, 11396.33972425, 11396.33972425, 11396.33972425],\n",
      "        [11398.17682043, 11398.17682043, 11398.17682043, 11398.17682043],\n",
      "        [11397.09021809, 11397.09021809, 11397.09021809, 11397.09021809],\n",
      "        [11397.75450241, 11397.75450241, 11397.75450241, 11397.75450241]]])]\n"
     ]
    }
   ],
   "source": [
    "print(best_parameter_per_set)\n",
    "print(losses_per_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for set 0\n",
      "Start penalized_logistic test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=1736243.8124513405\n",
      "Current iteration=1998, loss=1736216.2775540794\n",
      "Current iteration=2997, loss=1736188.743040396\n",
      "0.26150884500496335 0.40736314561816955\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=428294.4905167526\n",
      "Current iteration=1998, loss=428293.59964202764\n",
      "Current iteration=2997, loss=428292.7087749475\n",
      "0.6017398840433125 0.2732434524792204\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=732411.5030156521\n",
      "Current iteration=1998, loss=730648.1101130202\n",
      "Current iteration=2997, loss=728890.9214703498\n",
      "0.6114646118290623 0.26265322337903774\n",
      "Current iteration=0, loss=2692047.2431499376\n",
      "Current iteration=999, loss=2398689.5646239654\n",
      "Current iteration=1998, loss=2118721.352953305\n",
      "Current iteration=2997, loss=1859353.0567802708\n",
      "0.2906896468174823 0.39959252782014176\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=1036695.6374768509\n",
      "Current iteration=1998, loss=1019247.2633485694\n",
      "Current iteration=2997, loss=1002648.2025606612\n",
      "0.6590631358619908 0.20692705946207327\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=393273.248943312\n",
      "Current iteration=1998, loss=393272.6718942076\n",
      "Current iteration=2997, loss=393272.0948490425\n",
      "0.6213158834786644 0.25349825723518193\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=2110149.4499179986\n",
      "Current iteration=1998, loss=2110145.2425586716\n",
      "Current iteration=2997, loss=2110141.035199344\n",
      "0.6210391845056202 0.25383574681885684\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=1536924.5194022127\n",
      "Current iteration=1998, loss=1536578.1573354057\n",
      "Current iteration=2997, loss=1536231.8093580962\n",
      "0.44514176744949663 0.35968533585885165\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=468473.1505831723\n",
      "Current iteration=1998, loss=468440.1296245585\n",
      "Current iteration=2997, loss=468407.1117371094\n",
      "0.45654605149577077 0.357376796134624\n",
      "Current iteration=0, loss=1425164.3529966\n",
      "Current iteration=999, loss=1369015.7912018087\n",
      "Current iteration=1998, loss=1313071.6570921617\n",
      "Current iteration=2997, loss=1257357.8339891888\n",
      "0.3868367747628397 0.38375415837735294\n",
      "Current iteration=0, loss=1388342.3053906872\n",
      "Current iteration=999, loss=1060669.3914290501\n",
      "Current iteration=1998, loss=770644.1969761531\n",
      "Current iteration=2997, loss=553742.185059774\n",
      "0.45769345642786013 0.34899116600308366\n",
      "Current iteration=0, loss=537901.2752711584\n",
      "Current iteration=999, loss=514703.9467715677\n",
      "Current iteration=1998, loss=493838.8037868637\n",
      "Current iteration=2997, loss=475178.0777274689\n",
      "0.49213506440061 0.34107425184531376\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=419923.37626614096\n",
      "Current iteration=1998, loss=414524.21581504797\n",
      "Current iteration=2997, loss=409140.3637559766\n",
      "0.38681507273540966 0.37989782067247946\n",
      "Current iteration=0, loss=641537.3356550973\n",
      "Current iteration=999, loss=591497.796716022\n",
      "Current iteration=1998, loss=543535.8272062729\n",
      "Current iteration=2997, loss=498078.07725223276\n",
      "0.41348322116869957 0.37576228900513614\n",
      "Current iteration=0, loss=264393.1064760606\n",
      "Current iteration=999, loss=227669.97632533227\n",
      "Current iteration=1998, loss=213891.432930466\n",
      "Current iteration=2997, loss=207467.37172872838\n",
      "0.6357403000326186 0.23893021926756672\n",
      "Current iteration=0, loss=inf\n",
      "Current iteration=999, loss=510796.9245238451\n",
      "Current iteration=1998, loss=491501.7894097575\n",
      "Current iteration=2997, loss=473023.4304278176\n",
      "0.6185384443458763 0.25652744866124727\n",
      "Current iteration=0, loss=371238.96083365346\n",
      "Current iteration=999, loss=352321.84372007486\n",
      "Current iteration=1998, loss=335676.11088136607\n",
      "Current iteration=2997, loss=320743.7886469838\n",
      "0.7237750434746809 0.07005505121292456\n",
      "Current iteration=0, loss=2429623.0294913836\n",
      "Current iteration=999, loss=2132144.252381471\n",
      "Current iteration=1998, loss=1837545.5841627093\n",
      "Current iteration=2997, loss=1550406.0800409508\n",
      "0.26423426232865765 0.41139329857986295\n",
      "Current iteration=0, loss=1110529.6491145254\n",
      "Current iteration=999, loss=742654.96014331\n",
      "Current iteration=1998, loss=469924.7085957593\n",
      "Current iteration=2997, loss=330910.1913443757\n",
      "0.5872140537375179 0.28237204524451864\n",
      "Current iteration=0, loss=inf\n"
     ]
    }
   ],
   "source": [
    "#%% load data and process data\n",
    "# path =  \"C:/Users/jurri/OneDrive/Documenten/University/Exchange/ML/train\"\n",
    "# X, ids = load_data_features(path +\"/train.csv\")\n",
    "# y = load_results(path +\"/train.csv\")\n",
    "\n",
    "# # #X_test, ids_test = load_data_features(path +\"/test.csv\")\n",
    "\n",
    "# X_train, y_train, ids_train = X[:int(0.8*len(X)),:], y[:int(0.8*len(X))], ids[:int(0.8*len(X))]\n",
    "# X_test, y_test, ids_test = X[int(0.8*len(X)):,:], y[int(0.8*len(X)):], ids[int(0.8*len(X)):]\n",
    "\n",
    "# X_train, X_test_pro, y_train, y_test_pro, ids_tr_group, ids_test_group = process_data(X_train, \n",
    "#                                                     X_test, y_train, y_test, ids_train, ids_test)\n",
    "\n",
    "best_parameter_per_set, losses_per_set = find_parameters(X_train, y_train)\n",
    "y_result = []\n",
    "for i in range(1):\n",
    "    lambda_ = best_parameter_per_set[i][0,0]\n",
    "    degree = int(best_parameter_per_set[i][0,1])\n",
    "    X_train_ex = add_features(X_train[i], degree = degree)\n",
    "    X_test_pro_ex = add_features(X_test_pro[i], degree = degree)\n",
    "    show_result_logistic(X_train_ex, y_train[i], X_test_pro_ex, y_test_pro[i], lambda_, gamma = 10**-10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for set 0\n",
      "Start penalized_logistic test...\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33418.35594736119\n",
      "Current iteration=1998, loss=32946.80737836916\n",
      "Current iteration=2997, loss=32601.647528690773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33510.68712851258\n",
      "Current iteration=1998, loss=33029.095641885884\n",
      "Current iteration=2997, loss=32673.864366103073\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33817.637350342\n",
      "Current iteration=1998, loss=33514.659703567944\n",
      "Current iteration=2997, loss=33265.225666961254\n",
      "0.751188034615577 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33515.92405980315\n",
      "Current iteration=1998, loss=33045.91342653857\n",
      "Current iteration=2997, loss=32700.398558844783\n",
      "0.7452608388396358 0.00010002537662662905\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33418.35594736119\n",
      "Current iteration=1998, loss=32946.80737836916\n",
      "Current iteration=2997, loss=32601.647528690773\n",
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33510.68712851258\n",
      "Current iteration=1998, loss=33029.095641885884\n",
      "Current iteration=2997, loss=32673.864366103073\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33817.637350342\n",
      "Current iteration=1998, loss=33514.659703567944\n",
      "Current iteration=2997, loss=33265.225666961254\n",
      "0.751188034615577 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33515.92405980315\n",
      "Current iteration=1998, loss=33045.91342653857\n",
      "Current iteration=2997, loss=32700.398558844783\n",
      "0.7452608388396358 0.00010002537662662905\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33418.35594736119\n",
      "Current iteration=1998, loss=32946.80737836916\n",
      "Current iteration=2997, loss=32601.647528690773\n",
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33510.68712851258\n",
      "Current iteration=1998, loss=33029.095641885884\n",
      "Current iteration=2997, loss=32673.864366103073\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33817.637350342\n",
      "Current iteration=1998, loss=33514.659703567944\n",
      "Current iteration=2997, loss=33265.225666961254\n",
      "0.751188034615577 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33515.92405980315\n",
      "Current iteration=1998, loss=33045.91342653857\n",
      "Current iteration=2997, loss=32700.398558844783\n",
      "0.7452608388396358 0.00010002537662662905\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33418.35594736119\n",
      "Current iteration=1998, loss=32946.80737836916\n",
      "Current iteration=2997, loss=32601.647528690773\n",
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33510.68712851258\n",
      "Current iteration=1998, loss=33029.095641885884\n",
      "Current iteration=2997, loss=32673.864366103073\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33817.637350342\n",
      "Current iteration=1998, loss=33514.659703567944\n",
      "Current iteration=2997, loss=33265.225666961254\n",
      "0.751188034615577 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=33515.92405980315\n",
      "Current iteration=1998, loss=33045.91342653857\n",
      "Current iteration=2997, loss=32700.398558844783\n",
      "0.7452608388396358 0.00010002537662662905\n",
      "Highest F_score is: 10940.947321421601 for lambda: 1e-13\n",
      "Start penalized_logistic test...\n",
      "Current iteration=0, loss=41570.1158597216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\jurri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=999, loss=34080.45465515149\n",
      "Current iteration=1998, loss=34071.37203405078\n",
      "Current iteration=2997, loss=34062.62077113262\n",
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34183.80854629463\n",
      "Current iteration=1998, loss=34179.87851219306\n",
      "Current iteration=2997, loss=34176.02134235284\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34165.07958339976\n",
      "Current iteration=1998, loss=34158.23790524526\n",
      "Current iteration=2997, loss=34151.62649236898\n",
      "0.751188034615577 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34702.64538160622\n",
      "Current iteration=1998, loss=34685.278052026246\n",
      "Current iteration=2997, loss=34668.13448500001\n",
      "0.7416780600642506 0.01429396807490647\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34080.45465515149\n",
      "Current iteration=1998, loss=34071.37203405078\n",
      "Current iteration=2997, loss=34062.62077113262\n",
      "0.7435345905657545 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34183.80854629463\n",
      "Current iteration=1998, loss=34179.87851219306\n",
      "Current iteration=2997, loss=34176.02134235284\n",
      "0.7403831724275924 0.0\n",
      "Current iteration=0, loss=41570.1158597216\n",
      "Current iteration=999, loss=34165.07958339976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-34adf1d1cd19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_parameter_per_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_per_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_parameter_per_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_parameter_per_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-101f326388ef>\u001b[0m in \u001b[0;36mfind_parameters\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing for set\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         parameters, losses = hyper_optimizing(X_train[i], y_train[i],\n\u001b[1;32m---> 11\u001b[1;33m                                 methods, lambdas, degrees, 0.0000000001, 3000)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mbest_parameter_per_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-6930d569f969>\u001b[0m in \u001b[0;36mhyper_optimizing\u001b[1;34m(X_train, y_train, methods, lambdas, degrees, gamma, max_iter)\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                         loss_te, _ = cross_validation_logistic(y_train, X_train_ex, k_indices,\n\u001b[1;32m---> 46\u001b[1;33m                                                     k, lambda_, gamma, max_iter)\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mlosses_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mall_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-4864d9397b53>\u001b[0m in \u001b[0;36mcross_validation_logistic\u001b[1;34m(y, x, k_indices, k, lambda_, gamma, max_iter)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m999\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f180188a1966>\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-13b3023684d5>\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;34m\"\"\"computes the gradient of loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0minter_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minter_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_parameter_per_set, losses_per_set = find_parameters(X_train, y_train)\n",
    "y_result = []\n",
    "for i in range(1):\n",
    "    lambda_ = best_parameter_per_set[i][0,0]\n",
    "    degree = int(best_parameter_per_set[i][0,1])\n",
    "    X_train_ex = add_features(X_train[i], degree = degree)\n",
    "    X_test_pro_ex = add_features(X_test_pro[i], degree = degree)\n",
    "    show_result_logistic(X_train_ex, y_train[i], X_test_pro_ex, y_test_pro[i], lambda_, gamma = 0.000001)\n",
    "    \n",
    "    #w = ridge_regression(y_train[i], X_train_ex, lambda_)\n",
    "    #y_result.append(X_test_pro_ex @ w)\n",
    "\n",
    "# y_final = stitch_solution(X_test, y_result, ids_test_group, ids_test)\n",
    "# y_test = np.reshape(y_test, (len(y_test),1))\n",
    "# y_final = np.array(y_final)\n",
    "# quantify_result(y_final, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
