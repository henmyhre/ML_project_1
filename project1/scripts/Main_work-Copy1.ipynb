{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cupy as ccp\n",
    "import dask.array as cp\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    to_int = dict(s = 1,b = 0)\n",
    "    def convert(s):\n",
    "        return to_int.get(s.decode(\"utf-8\") , 0)\n",
    "    \n",
    "    data = cp.array(np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1],\n",
    "                                converters={1: convert}))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data_features(path_dataset):\n",
    "    \"\"\"load data features.\"\"\"\n",
    "    np_data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, \n",
    "                                usecols=tuple(range(2,32)))\n",
    "    data = cp.array(np_data)\n",
    "    print(np_data.shape, data.shape)\n",
    "    \n",
    "    ids = cp.array(np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[0]))\n",
    "    \n",
    "    return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    Extended = cp.empty((x.shape[0],0))\n",
    "    \n",
    "    for j in range(0, int(degree)+1):\n",
    "        for i in range(x.shape[1]):\n",
    "            Extended = cp.array(np.c_[Extended, x[:,i]**j])\n",
    "    \n",
    "    return Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    cp.random.seed(seed)\n",
    "    indices = cp.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return cp.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_filter(X, y, threshold = 0.01):\n",
    "    \"\"\"Removes features which are correlated with y with less than threshold\"\"\"\n",
    "    abs_corr = np.zeros(int(X.shape[1]))\n",
    "    for index, x in enumerate(X.T):\n",
    "        abs_corr[index] = np.abs(np.corrcoef(y,x.T)[0,1])\n",
    "        \n",
    "    quality = np.where(abs_corr > threshold)\n",
    "    \n",
    "    return cp.array(np.array(X)[:,quality[0]]), quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * cp.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    w -= gradient*gamma\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, mean = None, sigma = None):\n",
    "    \"\"\"Standardizes the data\"\"\"\n",
    "    if mean is None:\n",
    "        mean = cp.nanmean(data[data != -999], axis = 0)\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = cp.nanstd(data[data != -999], axis = 0)\n",
    "    \n",
    "    output = (data - mean)/sigma\n",
    "    \n",
    "    return output, mean, sigma\n",
    "\n",
    "def standardize_data(data, min_value = None, max_value = None):\n",
    "    \"\"\"maps data to [0,1] range\"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = cp.min(data, axis = 0)\n",
    "    \n",
    "    if max_value is None:\n",
    "        max_value = cp.max(data, axis = 0)\n",
    "        \n",
    "    output = (data - min_value)/(max_value - min_value)\n",
    "    \n",
    "    return output, min_value, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(data, y, ids):\n",
    "    \"\"\"Creates four subsets based on the number of jets,\n",
    "    which is 0, 1 and 2 or 3. 2 and 3 are put in one group,\n",
    "    since they keep same features and have similar correlation patterns\n",
    "    \"\"\"\n",
    "    data_subsets = []\n",
    "    y_subsets = []\n",
    "    ids_subsets = []\n",
    "    data = np.array(data)\n",
    "    for i in range(3):\n",
    "        if i ==2:\n",
    "            mask = data[:,22] >= i\n",
    "        else:\n",
    "            mask = data[:,22] == i\n",
    "        data_subsets.append(cp.array(data[mask]))\n",
    "        if y is not None:\n",
    "            y_subsets.append(y[mask])\n",
    "            \n",
    "        ids_subsets.append(ids[mask])\n",
    "        \n",
    "    return data_subsets, y_subsets, ids_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_variance(data, mask = None):\n",
    "    \"\"\"removes zero variance columns based on the subset\"\"\"\n",
    "    if mask is None:\n",
    "        variance = cp.var(data, axis = 0)\n",
    "        mask = cp.squeeze(~cp.logical_or([variance ==0],[cp.isnan(variance)]))\n",
    "    \n",
    "    return data[:, mask[:]], mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing(data, median = None):        \n",
    "    \"\"\"replaces nan by median value\"\"\"\n",
    "    if median is None:\n",
    "        median =[]\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            replace = float(ccp.median(ccp.asarray(data[mask,j])))\n",
    "            data[~mask,j] = replace\n",
    "            median.append(replace)\n",
    "    else:\n",
    "        for j in range(data.shape[1]):\n",
    "            mask = data[:,j] != -999\n",
    "            data[~mask,j] = median[j]\n",
    "\n",
    "    return data, median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X_train, X_test, y_train, y_test, ids_train, ids_test):\n",
    "    \"\"\"\n",
    "    Processes the test and training data by:\n",
    "    -splitting data with respect to jet number, creating three groups\n",
    "    -removing zero variance in each subgroup\n",
    "    -removing columns which are lowly correlated to y\n",
    "    -normalizing the data with mean and standard devation \n",
    "    -replacing -999 by median value of column\n",
    "    \"\"\"\n",
    "      \n",
    "    train_subsets, y_train, ids_train = create_subsets(X_train, y_train, ids_train)\n",
    "    test_subsets, _, ids_test = create_subsets(X_test, y_test, ids_test)\n",
    "    for i in range(3):\n",
    "        # change training sets\n",
    "        train_subsets[i], mean, sigma =  normalize_data(train_subsets[i], mean = None, sigma = None)\n",
    "        train_subsets[i], median = replace_missing(train_subsets[i], median = None)\n",
    "        train_subsets[i], mask = remove_zero_variance(train_subsets[i])\n",
    "        train_subsets[i], quality = correlation_filter(train_subsets[i], y_train[i], threshold = 0.01)\n",
    "        \n",
    "        #change test sets accordingly to training sets\n",
    "        test_subsets[i], _, _ =  normalize_data(test_subsets[i], mean, sigma)\n",
    "        test_subsets[i], _ = replace_missing(test_subsets[i], median)\n",
    "        test_subsets[i], _ = remove_zero_variance(test_subsets[i], mask)\n",
    "        test_subsets[i] = test_subsets[i][:, quality]\n",
    "    return train_subsets, test_subsets, y_train, y_test, ids_train, ids_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_terms(data):\n",
    "    \"\"\"Adds cross terms between columns\"\"\"\n",
    "    enriched_data = data\n",
    "    for x1 in data.T:\n",
    "        for x2 in data.T:\n",
    "            if cp.sum(x1 - x2) != 0:\n",
    "                enriched_data = cp.array(np.c_[np.array(enriched_data), x1*x2])\n",
    "                \n",
    "    return enriched_data      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_terms(data):\n",
    "    \"\"\"Adds log terms to data\"\"\"\n",
    "    extended = data\n",
    "    for column in data.T:\n",
    "        if cp.sum(column <= -1) == 0:\n",
    "            extended = cp.array(np.r_['-1,2,0', np.array(extended), np.log(1+ np.array(column))])\n",
    "        \n",
    "    return extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data, degree = None, sqrt = True, log = True, cross_terms = True):\n",
    "    \"\"\"\n",
    "    Adds following features to data set:\n",
    "    -log of features by log(1+x)\n",
    "    -sqrt of features\n",
    "    -polynomial extension of 0 up to degree\n",
    "    -cross terms of features\n",
    "    \"\"\" \n",
    "    #log\n",
    "    if log:\n",
    "        data = add_log_terms(data)\n",
    "        output = data\n",
    "    else:\n",
    "        output = cp.empty((data.shape[0],0))\n",
    "        \n",
    "    #polynomial\n",
    "    if degree is not None:\n",
    "        output = cp.array(np.c_[np.array(output), np.array(build_polynomial(data, degree))])\n",
    "      \n",
    "    # add sqrt\n",
    "    if sqrt:\n",
    "        output = cp.array(np.c_[np.array(output), np.sqrt(np.abs(np.array(data)))])\n",
    "        \n",
    "    \n",
    "    if cross_terms:\n",
    "        output = cp.array(np.c_[np.array(output), np.array(add_cross_terms(data))])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_solution(X_test, y_result, ids_test_group, ids_test):\n",
    "    \"\"\"\n",
    "    Puts found y values back in right order for the complete data matrix,\n",
    "    since it was split in four groups.\n",
    "    X_test: original, preprocessed test data\n",
    "    y_result: output of created model, list of three vectors containing predictions for group 1,2 and 3\n",
    "    \"\"\"\n",
    "    y_final=[]\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if ids_test[i] in ids_test_group[0]:\n",
    "            index = cp.where(ids_test_group[0] == ids_test[i])\n",
    "            y_final.append(y_result[0][index])\n",
    "            \n",
    "        elif ids_test[i] in ids_test_group[1]:\n",
    "            index = cp.where(ids_test_group[1] == ids_test[i])\n",
    "            y_final.append(y_result[1][index])\n",
    "            \n",
    "        elif ids_test[i] in ids_test_group[2]:\n",
    "            index = cp.where(ids_test_group[2] == ids_test[i])\n",
    "            y_final.append(y_result[2][index])\n",
    "            \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_optimizing(X_train, y_train, methods = [\"Ridge_regression\"],\n",
    "                     lambdas = [0.1], degrees = [1], gamma = 0.000001,  max_iter = 1):\n",
    "    \"\"\"Finds best lambda and degree to use on the given data, test possibilities are:\n",
    "    -Ridge regression\n",
    "    -Penalized Logistic regression\"\"\"\n",
    "    # Check method names are correct\n",
    "    if len([i for i in methods if i in [\"Ridge_regression\", \"Penalized_logistic\"]]) < len(methods):\n",
    "        raise NameError(\"At least one method is wrong\")\n",
    "        \n",
    "    all_losses = cp.zeros((len(methods), len(degrees), len(lambdas)))\n",
    "    best_parameters = cp.zeros((len(methods), 2))\n",
    "    \n",
    "    for degree_index, degree in enumerate(degrees):\n",
    "        X_train_ex = add_features(X_train, degree = degree)\n",
    "    \n",
    "        for method_index, method in enumerate(methods):\n",
    "            \n",
    "            if method == \"Ridge_regression\":\n",
    "                seed = 1\n",
    "                k_fold = 5\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start ridge regression test for degree\", str(degree),\"...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te = cross_validation_ridge(y_train, X_train_ex, k_indices, k, lambda_)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = cp.mean(cp.asarray(losses_te))\n",
    "                 \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[cp.argmin(all_losses[method_index, degree_index,:])]\n",
    "                print(\"Lowest loss is:\",min(all_losses[method_index, degree_index,:]), \"for lambda:\", min_lambda)\n",
    "                \n",
    "                \n",
    "            elif method == \"Penalized_logistic\":\n",
    "                #less k-fold for reason of speed\n",
    "                seed = 1\n",
    "                k_fold = 1\n",
    "                k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "                print(\"Start penalized_logistic test...\")\n",
    "                for index, lambda_ in enumerate(lambdas):\n",
    "                    losses_te = []\n",
    "                    for k in range(k_fold):\n",
    "                        loss_te, _ = cross_validation_logistic(y_train, X_train_ex, k_indices,\n",
    "                                                    k, lambda_, gamma = 0.000001, max_iter = max_iter)\n",
    "                        losses_te.append(loss_te)\n",
    "                    all_losses[method_index, degree_index, index] = cp.mean(cp.asarray(losses_te))\n",
    "                \n",
    "                # Show percantage of correct results for this degree\n",
    "                min_lambda = lambdas[cp.argmin(all_losses[method_index, degree_index,:])]\n",
    "                print(\"Lowest loss is:\", min(all_losses[method_index, degree_index,:]),\"for lambda:\", min_lambda)\n",
    "                \n",
    "            \n",
    "    for k  in range(len(methods)):\n",
    "        min_loss = cp.argmin(all_losses[k,:,:], axis = 0)\n",
    "        best_parameters[k,0] = lambdas[min_loss[0]]\n",
    "        best_parameters[k,1] = degrees[min_loss[1]]\n",
    "        \n",
    "    return best_parameters, all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_result(y_found, y_real):\n",
    "    y_found[y_found<0.5]=0\n",
    "    y_found[y_found>=0.5]=1\n",
    "    summ = y_found + y_real\n",
    "    TP = sum(summ == 2)\n",
    "    TN = sum(summ == 0)\n",
    "    diff = y_found - y_real\n",
    "    FP = sum(diff == 1)\n",
    "    FN = sum(diff == -1)\n",
    "    accuracy = (TP+TN)/(TP +TN +FP + FN)\n",
    "    F_score = TP/(TP + 0.5 * (FP +FN))\n",
    "    recall = TP/(TP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    return precision, recall, F_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(cp.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "\n",
    "    w = ridge_regression(y_train, x_train, lambda_)\n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test, x_test, w))\n",
    "    # Calculate F_score, seems more reliable to compare different degrees\n",
    " #   y_new = x_test @ w\n",
    "  #  precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    \n",
    "    return loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k,lambda_, gamma,max_iter):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # split according to k_indices\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_test = x[k_indices[k], :]\n",
    "    tr_indice = k_indices[~(cp.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice, :]\n",
    "    \n",
    "    w = cp.zeros((x.shape[1], 1))\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y_train, x_train, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 999 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # check loss actually decreases, if not decrease gamma\n",
    "        if iter > 0:\n",
    "            if loss > losses[-1]:\n",
    "                gamma = gamma/2\n",
    "            \n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and ccp.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    # calculate the loss for train and test data:\n",
    "    loss_te = calculate_loss(y_test, x_test, w)\n",
    "    \n",
    "    # Calculate F_score, seems more reliable to compare different degrees\n",
    "    #y_new = x_test @ w\n",
    "    #precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    \n",
    "    return loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"ridge regression\"\"\"\n",
    "    if len(tx.shape) > 1:\n",
    "        w = cp.linalg.solve(tx.T @ tx + (2*tx.shape[0]*lambda_)*cp.identity(tx.shape[1]), tx.T @ y)\n",
    "    else:\n",
    "        w = 1/(tx.T @ tx + lambda_) * tx.T @ y                        \n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"applies the sigmoid function on t.\"\"\"\n",
    "    return 1/(1+cp.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"computes the loss: negative log likelihood.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    z = tx @ w\n",
    "    a = cp.sum(cp.log(1 + cp.exp(z)))\n",
    "    b = inter_y.T @ z\n",
    "    loss = a - b\n",
    "    return cp.squeeze(loss)\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"computes the gradient of loss.\"\"\"\n",
    "    inter_y = y.reshape(len(y),1)\n",
    "    gradient = tx.T @ (sigmoid(tx @ w) - inter_y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_ridge(X_train, y_train, X_test, y_test, lambda_):\n",
    "    \"\"\"prints accuracy of ridge regression\"\"\"\n",
    "    w = ridge_regression(y_train, X_train, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n",
    "    print(\"Precision is:\",str(precision), \"and recall is:\",str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_logistic(X_train, y_train, X_test, y_test, lambda_, gamma = 0.000001):\n",
    "    \"\"\"prints accuracy of logistic regression\"\"\"\n",
    "    w = cp.zeros((x.shape[1], 1))\n",
    "    _, w = learning_by_penalized_gradient(y_train, X_train, w, gamma, lambda_)\n",
    "    y_new = X_test @ w\n",
    "    precision, recall, F_score, accuracy = quantify_result(y_new, y_test)\n",
    "    print(\"Accuracy of the predictions is:\",\n",
    "          str(accuracy), \" and F-score is:\", str(F_score), \"with lambda:\",lambda_)\n",
    "    print(\"Precision is:\",str(precision), \"and recall is:\",str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parameters(X_train, y_train):\n",
    "    \"\"\"Finding best parameters and losses per set\"\"\"\n",
    "    best_parameter_per_set = []\n",
    "    losses_sets =[]\n",
    "    methods = [\"Ridge_regression\"]\n",
    "    lambdas = [cp.array(np.logspace(-13,-9,6)), cp.array(np.logspace(-13,-5,9)), cp.array(np.logspace(-13,-9,6))]\n",
    "    degrees = [cp.linspace(3,9,7, dtype = int), cp.linspace(6,14,9, dtype = int), cp.linspace(9,17,8, dtype = int)]\n",
    "    for i in range(3):\n",
    "        print(\"Testing for set\",i)\n",
    "        parameters, losses = hyper_optimizing(X_train[i], y_train[i],\n",
    "                                methods, lambdas[i], degrees[i])\n",
    "        \n",
    "        best_parameter_per_set.append(parameters)\n",
    "        losses_sets.append(losses)\n",
    "    \n",
    "    return best_parameter_per_set, losses_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000, 30)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6b63b6e6e09a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#X_train = cp.asarray(X_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"/test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-02be61f9fb5d>\u001b[0m in \u001b[0;36mload_results\u001b[1;34m(path_dataset)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mto_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     data = cp.array(np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1],\n\u001b[0m\u001b[0;32m      8\u001b[0m                                 converters={1: convert}))\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   2036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m         \u001b[1;31m# Parse each line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2038\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfhd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2039\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0mnbvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% load data and process data\n",
    "path =  \"data\"\n",
    "X_train, ids_train = load_data_features(path +\"/train.csv\")\n",
    "#X_train = cp.asarray(X_train)\n",
    "y_train = load_results(path +\"/train.csv\")\n",
    "\n",
    "X_test, ids_test = load_data_features(path +\"/test.csv\")\n",
    "print(X_train.shape)\n",
    "#X_train, y_train, ids_train = X[:int(0.8*len(X)),:], y[:int(0.8*len(X))], ids[:int(0.8*len(X))]\n",
    "#X_test, y_test, ids_test = X[int(0.8*len(X)):,:], y[int(0.8*len(X)):], ids[int(0.8*len(X)):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_pro, y_train, _, ids_tr_group, ids_test_group = process_data(X_train, \n",
    "                                                    X_test, y_train, None, ids_train, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameter_per_set, losses_per_set = find_parameters(X_train, y_train)\n",
    "y_result = []\n",
    "for i in tqdm(range(3)):\n",
    "    lambda_ = best_parameter_per_set[i][0,0]\n",
    "    degree = int(best_parameter_per_set[i][0,1])\n",
    "    X_train_ex = add_features(X_train[i], degree = degree)\n",
    "    X_test_pro_ex = add_features(X_test_pro[i], degree = degree)\n",
    "    #show_result_ridge(X_train_ex, y_train[i], X_test_pro_ex, y_test_pro[i], lambda_)\n",
    "    \n",
    "    w = ridge_regression(y_train[i], X_train_ex, lambda_)\n",
    "    y_result.append(X_test_pro_ex @ w)\n",
    "\n",
    "y_final = stitch_solution(X_test, y_result, ids_test_group, ids_test)\n",
    "y_test = cp.reshape(y_test, (len(y_test),1))\n",
    "y_final = cp.array(y_final)\n",
    "quantify_result(y_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
